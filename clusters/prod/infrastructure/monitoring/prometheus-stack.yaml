---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: prometheus-community
  namespace: monitoring
spec:
  interval: 24h
  url: https://prometheus-community.github.io/helm-charts
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 30m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: "*"
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: monitoring
      interval: 12h
  values:
    alertmanager:
      enabled: true
      alertmanagerSpec:
        replicas: 1  # Reduce from 2 to 1 for single-node cluster
        # Disable storage for now - enable in production
        # storage:
        #   volumeClaimTemplate:
        #     spec:
        #       storageClassName: local-path
        #       accessModes: ["ReadWriteOnce"]
        #       resources:
        #         requests:
        #           storage: 50Gi
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 50m  # Reduce from 100m to 50m
            memory: 64Mi  # Reduce from 128Mi to 64Mi

    grafana:
      enabled: true
      replicas: 1
      persistence:
        enabled: false
        # For production, enable persistence and install storage provisioner:
        # enabled: true
        # type: pvc
        # storageClassName: local-path
        # accessModes:
        #   - ReadWriteOnce
        # size: 10Gi
      resources:
        limits:
          cpu: 100m  # Reduce from 200m to 100m
          memory: 128Mi  # Reduce from 200Mi to 128Mi
        requests:
          cpu: 50m  # Reduce from 100m to 50m
          memory: 64Mi  # Reduce from 128Mi to 64Mi
      adminPassword: "admin"
      grafana.ini:
        server:
          root_url: https://grafana.malachowski.me
        security:
          admin_user: admin
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          # Kubernetes cluster monitoring dashboard
          kubernetes-cluster-monitoring:
            gnetId: 7249
            revision: 1
            datasource: Prometheus
          # Node exporter dashboard
          node-exporter:
            gnetId: 1860
            revision: 37
            datasource: Prometheus
          # Traefik dashboard
          traefik:
            gnetId: 4475
            revision: 5
            datasource: Prometheus

    kubeApiServer:
      enabled: true

    kubelet:
      enabled: true
      serviceMonitor:
        metricRelabelings:
          - action: replace
            sourceLabels:
              - node
            targetLabel: instance

    kubeControllerManager:
      enabled: true

    coreDns:
      enabled: true

    kubeDns:
      enabled: false

    kubeEtcd:
      enabled: true

    kubeScheduler:
      enabled: true

    kubeProxy:
      enabled: true

    kubeStateMetrics:
      enabled: true

    nodeExporter:
      enabled: true
      serviceMonitor:
        relabelings:
          - action: replace
            regex: (.*)
            replacement: $1
            sourceLabels:
              - __meta_kubernetes_pod_node_name
            targetLabel: kubernetes_node

    prometheusOperator:
      enabled: true
      resources:
        limits:
          cpu: 100m  # Reduce from 200m to 100m
          memory: 128Mi  # Reduce from 200Mi to 128Mi
        requests:
          cpu: 50m  # Reduce from 100m to 50m
          memory: 64Mi  # Reduce from 100Mi to 64Mi

    prometheus:
      enabled: true
      prometheusSpec:
        replicas: 1  # Reduce from 2 to 1 for single-node cluster
        shards: 1
        retention: 7d  # Reduce retention from 30d to 7d
        retentionSize: 2GiB  # Reduce from 15GiB to 2GiB
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        scrapeInterval: 60s  # Reduce scraping frequency
        evaluationInterval: 60s
        enableAdminAPI: false
        # Disable storage for now - enable in production
        # storageSpec:
        #   volumeClaimTemplate:
        #     spec:
        #       storageClassName: local-path
        #       accessModes: ["ReadWriteOnce"]
        #       resources:
        #         requests:
        #           storage: 20Gi
        resources:
          limits:
            cpu: 500m  # Reduce from 2000m to 500m
            memory: 1Gi  # Reduce from 8Gi to 1Gi
          requests:
            cpu: 200m  # Reduce from 1000m to 200m
            memory: 512Mi  # Reduce from 4Gi to 512Mi

    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: true
        configReloaders: true
        general: true
        k8s: true
        kubeApiserver: true
        kubeApiserverAvailability: true
        kubeApiserverSlos: true
        kubelet: true
        kubeProxy: true
        kubePrometheusGeneral: true
        kubePrometheusNodeRecording: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeScheduler: true
        kubeStateMetrics: true
        network: true
        node: true
        nodeExporterAlerting: true
        nodeExporterRecording: true
        prometheus: true
        prometheusOperator: true
